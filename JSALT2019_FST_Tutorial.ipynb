{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological analysis with FSTs\n",
    "The following is a brief and basic tutorial on how to construct a **morphological analyzer** for a language using finite-state techniques. A toy grammar of English noun and verb inflections is built step-by-step to illustrate overall design issues. While the grammar is small, much larger grammars can be built using the same design principles. This tutorial uses the [Helsinki Finite-State Transducer toolkit](http://hfst.github.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hfst\n",
    "import fstutils as fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function remove_epsilons in module fstutils:\n",
      "\n",
      "remove_epsilons(string, epsilon='@_EPSILON_SYMBOL_@')\n",
      "    Removes the epsilon transitions from the string along a path from hfst.\n",
      "    \n",
      "    Args:\n",
      "        string (str): The string (e.g. input path, output form) from which the epsilons should be deleted.\n",
      "        epsilon (str, optional):  The epsilon string to remove. Defaults to the default setting in hfst,\n",
      "        '@_EPSILON_SYMBOL_@'. Pass this only if you've redefined the epsilon symbol string in hfst.\n",
      "    \n",
      "    Returns:\n",
      "        str: The desired string, without epsilons\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fst.remove_epsilons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "The construction of the final transducer is broken down into two large components:\n",
    "\n",
    "- A lexicon\n",
    "- Alternation rules\n",
    "\n",
    "## Lexicon\n",
    "\n",
    "The lexicon component is a transducer that:\n",
    "\n",
    "- Accepts as input the valid stems/lemmas of the language, followed by a legal sequence of *tags*.\n",
    "- Produces as output an intermediate form in which the tags are replaced by the morphemes that they correspond to.\n",
    "- May produce additional symbols in the output, such as special symbols that serve to mark the presence of morpheme boundaries.\n",
    "\n",
    "For example, in the analyzer to be constructed, the lexicon FST performs the following mappings:\n",
    "```\n",
    "c a t +N +Pl      w a t c h +N +Pl      w a t c h +V +3P +Sg     (input side)\n",
    "c a t ^  s        w a t c h ^  s        w a t c h ^  s           (output side)\n",
    "```\n",
    "\n",
    "There are two things to note here:\n",
    "\n",
    "1. We use the symbol `^` to mark a morpheme boundary.\n",
    "2. While each letter in the stem is represented by its own symbol (`w`, `a`, `t`, `c`, `h`, etc.), each complete tag is a *multicharacter symbol* (`+N`, `+Pl`, etc.). The spaces in the example above show the symbol boundaries to illustrate this.\n",
    "\n",
    "The lexicon transducer is written in a formalism called *lexc*.\n",
    "\n",
    "## Alternation rules\n",
    "\n",
    "The role of the alternation rules is to modify the output of the lexicon transducer according to orthographic, phonological, and morphophonological rules and conventions. So far, for example, we've assumed that English nouns can be pluralized by concatenating the morpheme *-s* to the stem (`cat` → `cats`).  However, noun stems that end in a sibilant take the allomorph *-es* (`watch` → `watches`). A way to describe the process of forming correct nouns is to always represent the plural as the morpheme *-s* and then subject these word forms to alternation rules that insert an *e* if the stem ends in a sibilant. This is one of the tasks of the rules component: to produce the valid surface forms from the intermediate forms output by the lexicon transducer.\n",
    "Since rule FSTs that are conditioned by their environment are very difficult to construct by hand, we use the replacement rules formalism to compile the necessary rules into FSTs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine the lexicon FST and the various FSTs that encode alternation rules into one large transducer that acts like a cascade. This single large transducer has the same effect as providing an input to the lexicon transducer, taking its output and feeding it into the first rule transducer, taking its output and feeding it into the next rule transducer, and so on.\n",
    "This cascade is accomplished by the regular expression composition operator (`.o.`). Suppose we have the lexicon transducer in an FST named `Lexicon` and the various alternation rules as FSTs named `Rule1`, ..., `RuleN`. We can issue the regular expression\n",
    "```\n",
    "Lexicon .o. Rule1 .o. Rule2 .o. ... .o. RuleN ;\n",
    "```\n",
    "and produce a single transducer that is the composite of the different rule transducers and the lexicon transducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = fst.Definitions({\n",
    "    \"V\": \"[a|i|e|o|u]\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = hfst.compile_lexc_file('english.lexc')\n",
    "\n",
    "consonantduplication = hfst.regex(defs.replace('g -> g g || _ \"^\" [i n g | e d]'))\n",
    "edeletion = hfst.regex('e -> 0 || _ \"^\" [ i n g | e d ]')\n",
    "einsertion = hfst.regex('[..] -> e || s | z | x | c h | s h _ \"^\" s')\n",
    "yreplacement = hfst.regex('y -> i e || _ \"^\" s ,, y-> i || _ \"^\" e d')\n",
    "kinsertion = hfst.regex(defs.replace('[..] -> k || V c _ \"^\" [e d | i n g]'))\n",
    "cleanup = hfst.regex('\"^\" -> 0')\n",
    "\n",
    "# be careful, since composition is done in place, rerunning composes without redefining the fst from scratch will make mega-fsts\n",
    "\n",
    "grammar.compose(consonantduplication)\n",
    "grammar.compose(einsertion)\n",
    "grammar.compose(edeletion)\n",
    "grammar.compose(yreplacement)\n",
    "grammar.compose(kinsertion)\n",
    "grammar.compose(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'panicked'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst.lookup(grammar, 'panic+V+Past')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beg+V:\n",
      " beg\n",
      "beg+V+3P+Sg:\n",
      " begs\n",
      "beg+V+Past:\n",
      " begged\n",
      "beg+V+PastPart:\n",
      " begged\n",
      "beg+V+PresPart:\n",
      " begging\n",
      "cat+N+Sg:\n",
      " cat\n",
      "cat+N+Pl:\n",
      " cats\n",
      "city+N+Pl:\n",
      " cities\n",
      "city+N+Sg:\n",
      " city\n",
      "fox+N+Sg:\n",
      " fox\n",
      "fox+V:\n",
      " fox\n",
      "fox+V+Past:\n",
      " foxed\n",
      "fox+V+PastPart:\n",
      " foxed\n",
      "fox+V+PresPart:\n",
      " foxing\n",
      "fox+N+Pl:\n",
      " foxes\n",
      "fox+V+3P+Sg:\n",
      " foxes\n",
      "make+V+Past:\n",
      " maked\n",
      "make+V+PastPart:\n",
      " maked\n",
      "make+V+PresPart:\n",
      " making\n",
      "make+V:\n",
      " make\n",
      "make+V+3P+Sg:\n",
      " makes\n",
      "panic+N+Sg:\n",
      " panic\n",
      "panic+N+Pl:\n",
      " panics\n",
      "panic+V:\n",
      " panic\n",
      "panic+V+3P+Sg:\n",
      " panics\n",
      "panic+V+Past:\n",
      " panicked\n",
      "panic+V+PastPart:\n",
      " panicked\n",
      "panic+V+PresPart:\n",
      " panicking\n",
      "try+V+Past:\n",
      " tried\n",
      "try+V+PastPart:\n",
      " tried\n",
      "try+N+Pl:\n",
      " tries\n",
      "try+V+3P+Sg:\n",
      " tries\n",
      "try+N+Sg:\n",
      " try\n",
      "try+V:\n",
      " try\n",
      "try+V+PresPart:\n",
      " trying\n",
      "watch+N+Sg:\n",
      " watch\n",
      "watch+V:\n",
      " watch\n",
      "watch+V+Past:\n",
      " watched\n",
      "watch+V+PastPart:\n",
      " watched\n",
      "watch+V+PresPart:\n",
      " watching\n",
      "watch+N+Pl:\n",
      " watches\n",
      "watch+V+3P+Sg:\n",
      " watches\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fst.pairs(grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
