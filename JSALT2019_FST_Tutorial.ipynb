{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological analysis with FSTs\n",
    "The following is a brief and basic tutorial on how to construct a **morphological analyzer** for a language using finite-state techniques. A toy grammar of English noun and verb inflections is built step-by-step to illustrate overall design issues. While the grammar is small, much larger grammars can be built using the same design principles. This tutorial uses the [Helsinki Finite-State Transducer toolkit](http://hfst.github.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hfst\n",
    "import fstutils as fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(fst.remove_epsilons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "The construction of the final transducer is broken down into two large components:\n",
    "\n",
    "- A lexicon\n",
    "- Alternation rules\n",
    "\n",
    "## Lexicon\n",
    "\n",
    "The lexicon component is a transducer that:\n",
    "\n",
    "- Accepts as input the valid stems/lemmas of the language, followed by a legal sequence of **tags**.\n",
    "- Produces as output an intermediate form in which the tags are replaced by the morphemes that they correspond to.\n",
    "- May produce additional symbols in the output, such as special symbols that serve to mark the presence of morpheme boundaries.\n",
    "\n",
    "For example, in the analyzer to be constructed, the lexicon FST performs the following mappings:\n",
    "```\n",
    "c a t +N +Pl      w a t c h +N +Pl      w a t c h +V +3P +Sg     (input side)\n",
    "c a t ^  s        w a t c h ^  s        w a t c h ^  s           (output side)\n",
    "```\n",
    "\n",
    "There are two things to note here:\n",
    "\n",
    "1. We use the symbol `^` to mark a morpheme boundary.\n",
    "2. While each letter in the stem is represented by its own symbol (`w`, `a`, `t`, `c`, `h`, etc.), each complete tag is a **multicharacter symbol** (`+N`, `+Pl`, etc.). The spaces in the example above show the symbol boundaries to illustrate this.\n",
    "\n",
    "The lexicon transducer is written in a formalism called **lexc**.\n",
    "\n",
    "## Alternation rules\n",
    "\n",
    "The role of the alternation rules is to modify the output of the lexicon transducer according to orthographic, phonological, and morphophonological rules and conventions. So far, for example, we've assumed that English nouns can be pluralized by concatenating the morpheme *-s* to the stem (`cat` → `cats`).  However, noun stems that end in a sibilant take the allomorph *-es* (`watch` → `watches`). A way to describe the process of forming correct nouns is to always represent the plural as the morpheme *-s* and then subject these word forms to alternation rules that insert an *e* if the stem ends in a sibilant. This is one of the tasks of the rules component: to produce the valid surface forms from the intermediate forms output by the lexicon transducer.\n",
    "\n",
    "Since rule FSTs that are conditioned by their environment are very difficult to construct by hand, we use the replace rules formalism to compile the necessary rules into FSTs.\n",
    "\n",
    "### Replace rule basics\n",
    "\n",
    "Replace rules are often used when modeling morphophonology.  These have the following basic format.\n",
    "\n",
    "    LHS -> RHS || LC _ RC ;\n",
    "\n",
    "where `LHS` is left-hand-side, `RHS` is right-hand-side, `LC` is left-context, and `RC` is right-context.\n",
    "\n",
    "A regular expression such as:\n",
    "\n",
    "    regex a -> b || c _ d ;\n",
    "\n",
    "would create a transducer that converts all instances of `a` to `b`, but only if that `a` occurs between `c` and `d`.\n",
    "\n",
    "The `LC` and `RC` are optional, so you could say:\n",
    "\n",
    "    regex a -> 0 || _ b ;\n",
    "\n",
    "creating a transducer that deletes `a`-symbols when they occur before a `b`.\n",
    "\n",
    "You can also drop the context altogether, and say:\n",
    "\n",
    "    regex x -> y ;\n",
    "\n",
    "Which creates a transducer that maps all `x`-symbols to `y`, always, passing everything else through untouched.\n",
    "\n",
    "#### Word boundaries in rewrite rules\n",
    "\n",
    "The special symbol `.#.` is used to refer to edges of words.  For example, the following transducer deletes all `x`-symbols, but only at the beginning of a word.  Everything else is passed through as-is.\n",
    "\n",
    "    regex x -> 0 || .#. _ ;\n",
    "\n",
    "#### Epenthesis (insertion) rules\n",
    "\n",
    "There is a special type of rule, called epenthesis rules is used whenever you want to insert something (from nothing) in a special position.  This is denoted by `[..]` as the `LHS`.  For example:\n",
    "\n",
    "    regex [..] -> x || y _ z ;\n",
    "\n",
    "would result in a transducer that inserts `x` between `y` and `z`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon\n",
    "\n",
    "As mentioned above, the lexicon script is written in the **lexc** formalism and stored in a text file, which we call `english.lexc`.\n",
    "\n",
    "## Features\n",
    "    \n",
    "We want to include the following features in the grammar.\n",
    "\n",
    "- Nouns: singular (*cat*) vs. plural (*cats*)\n",
    "- Verbs: infinitive (*watch*), 3rd person singular (*watches*), past tense (*watched*), past participle (*watched*), and present participle (*watching*)\n",
    "\n",
    "We also want to use the following tags for marking the above grammatical information:\n",
    "\n",
    "- `+N` for nouns\n",
    "- `+V` for verbs\n",
    "- `+3P` for third person\n",
    "- `+Sg` for singular forms\n",
    "- `+Pl` for plural forms\n",
    "- `+Past` for past tense\n",
    "- `+PastPart` for past participle\n",
    "- `+PresPart` for present participle\n",
    "\n",
    "## lexc\n",
    "\n",
    "The `lexc` formalism operates under the simple notion of continuation classes. We declare a set of labeled lexicons, the content of those lexicons, and rules which dictate how lexicon entries are concatenated.\n",
    "\n",
    "Here, we provide a line-by-line analysis of `english.lexc`.\n",
    "\n",
    "First, we need to declare the multicharacter symbols:\n",
    "\n",
    "```\n",
    "Multichar_Symbols +N +V +PastPart +Past +PresPart +3P +Sg +Pl\n",
    "```\n",
    "\n",
    "Then, we declare a lexicon called `Root` and provide it with two entries:\n",
    "\n",
    "```\n",
    "LEXICON Root\n",
    "\n",
    "Noun ;\n",
    "Verb ;\n",
    "```\n",
    "\n",
    "Both entries are empty (i.e., there is no left-hand side), and by choosing one of these, we can move to either the `Noun` or the `Verb` lexicon.\n",
    "\n",
    "The `Noun` lexicon looks as follows:\n",
    "\n",
    "```\n",
    "LEXICON Noun\n",
    "\n",
    "cat   Nstem;\n",
    "city  Nstem;\n",
    "fox   Nstem;\n",
    "panic Nstem;\n",
    "try   Nstem;\n",
    "watch Nstem;\n",
    "```\n",
    "\n",
    "Here, we have six entries.  All of these entries continue to the `Nstem` lexicon.\n",
    "\n",
    "The `Verb` lexicon also has six entries, and all of them continue to the `Vinf` lexicon.\n",
    "\n",
    "```\n",
    "LEXICON Verb\n",
    "\n",
    "beg   Vinf;\n",
    "fox   Vinf;\n",
    "make  Vinf;\n",
    "panic Vinf;\n",
    "try   Vinf;\n",
    "watch Vinf;\n",
    "```\n",
    "\n",
    "The `Nstem` lexicon has two entries:\n",
    "\n",
    "```\n",
    "LEXICON Nstem\n",
    "\n",
    "+N+Sg:0   #;\n",
    "+N+Pl:^s  #;\n",
    "```\n",
    "\n",
    "Unlike in the previous lexicons, both entries specify the input string and the output string separately. For example, the entry `+N+Sg:0` specifies that the input side contains `+N+Sg` and the output side the empty string. The continuation lexicon for both is `#`, meaning end-of-word. So if we choose from the `Root` lexicon to enter the `Noun` lexicon, and from there choose the `cat` entry, and in the `Ninf` lexicon choose the `+N+Sg:0` entry, we will construct the input-output pairing:\n",
    "\n",
    "```\n",
    "c a t +N +Sg\n",
    "c a t\n",
    "```\n",
    "\n",
    "In the `Vinf` lexicon, we have a few more entries, one for each verb form:\n",
    "\n",
    "```\n",
    "LEXICON Vinf\n",
    "\n",
    "+V:0             #;\n",
    "+V+3P+Sg:^s      #;\n",
    "+V+Past:^ed      #;\n",
    "+V+PastPart:^ed  #;\n",
    "+V+PresPart:^ing #;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compile the `english.lexc` script into a transducer and label it for subsequent use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = hfst.compile_lexc_file('english.lexc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternation rules\n",
    "\n",
    "We now turn to the alternation rules component. The idea is to construct a set of ordered rule transducers that modify the intermediate forms output by the lexicon component. At the very least, we need to remove the `^` symbol which is used to separate morpheme boundaries. This is in fact the **last** rule transducer in the cascade. There are a number of phonological and orthographic rules that are needed first.\n",
    "\n",
    "## Natural classes\n",
    "\n",
    "Before we define the rules, we need to define the class of vowels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = fst.Definitions({\n",
    "    \"V\": \"[ a | i | e | o | u ]\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "### e-deletion\n",
    "\n",
    "Stems that end in a silent `e` drop the `e` with certain suffixes (`ing` and `ed` in our case). For example, `make` → `making`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edeletion = hfst.regex('e -> 0 || _ \"^\" [ i n g | e d ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the rule to the output of the lexicon component, we get changes like:\n",
    "\n",
    "```\n",
    "m a k e +V +PresPart        (lexicon input)\n",
    "m a k e ^  i         n g    (lexicon output)\n",
    "m a k   ^  i         n g    (after e-deletion)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e-insertion\n",
    "\n",
    "This rule applies when a stem ends in a sibilant and is followed by the plural morpheme `s` (`watch` → `watches`).  Sibilants can be defined orthographically by `s`, `z`, `x`, `c h`, and `s h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einsertion = hfst.regex('[..] -> e || [ s | z | x | c h | s h ] _ \"^\" s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `[..]` construction on the left-hand side. Here, using an epsilon (`0`) would insert a potentially infinite number of `e` symbols. The special construction `[..]` forces epsilon insertions to restrict themselves to one insertion per potential insertion site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y-replacement\n",
    "\n",
    "This rule, which acts in constructions such as `try` → `tries` and `try` → `tried`, is perhaps best broken into two parts. Consider the corresponding input-intermediate form pairings:\n",
    "\n",
    "```\n",
    "(1)                   (2)\n",
    "t r y   +V +3P +Sg    t r y +V +PastPart      (lex in)\n",
    "t r y   ^  s          t r y ^  e          d   (lex out)\n",
    "t r i e ^  s          t r i ^  e          d   (desired rule output)\n",
    "```\n",
    "\n",
    "Case (1) requires changing changing the `y` into `i e`, whereas case (2) requires only changing the `y` into `i`. We can accomplish this by having two parallel rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yreplacement = hfst.regex('y -> i e || _ \"^\" s ,, y-> i || _ \"^\" e d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `,,` separates the two rules. These rules operate simultaneously and independently. For this particular case, it doesn't matter whether the two rules apply simultaneously or one after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-insertion\n",
    "\n",
    "Verbs that end in a `c` add a `k` before the morpheme boundary if followed by an suffix beginning with a vowel. For example, `panic` → `panicking` `panicked`. This rule relies on us having defined V as our set of vowel symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinsertion = hfst.regex(defs.replace('[..] -> k || V c _ \"^\" [e d | i n g]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consonant doubling (gemination)\n",
    "\n",
    "Consonant doubling is really the same phenomenon as **k-insertion**. We double final consonants in the stem in certain environments: `beg` → `begging`, `run` → `runnable`, etc. Our lexicon contains only one such word (*beg*), so we can just use one rule. For a more thorough treatment, we'd also need to add rules for the other consonants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consonantduplication = hfst.regex('g -> g g || _ \"^\" [i n g | e d]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary symbol removal\n",
    "\n",
    "As the last rule, we need to remove the auxiliary `^`, whose presence is necessary for the alternation rules. The last rule removes these and produces valid surface forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup = hfst.regex('\"^\" -> 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the grammar\n",
    "\n",
    "We combine the lexicon FST and the various FSTs that encode alternation rules into one large transducer that acts like a cascade. This single large transducer has the same effect as providing an input to the lexicon transducer, taking its output and feeding it into the first rule transducer, taking its output and feeding it into the next rule transducer, and so on.\n",
    "\n",
    "Normally, this cascade is accomplished by the regular expression composition operator (`.o.`). Suppose we have the lexicon transducer in an FST named `Lexicon` and the various alternation rules as FSTs named `Rule1`, ..., `RuleN`. We can issue the regular expression\n",
    "```\n",
    "Lexicon .o. Rule1 .o. Rule2 .o. ... .o. RuleN ;\n",
    "```\n",
    "and produce a single transducer that is the composite of the different rule transducers and the lexicon transducer.\n",
    "\n",
    "However, in HFST, we compile the grammar differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful here.\n",
    "# Since composition is done in place, rerunning composes without redefining the FST from scratch will make mega-FSTs.\n",
    "\n",
    "grammar.compose(consonantduplication)\n",
    "grammar.compose(einsertion)\n",
    "grammar.compose(edeletion)\n",
    "grammar.compose(yreplacement)\n",
    "grammar.compose(kinsertion)\n",
    "grammar.compose(cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fst.lookup(grammar, 'panic+V+Past')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list all the input/output pairs of the words with the `pairs` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fst.pairs(grammar))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
