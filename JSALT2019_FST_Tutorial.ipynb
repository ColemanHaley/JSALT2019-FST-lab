{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological analysis with FSTs\n",
    "The following is a brief and basic tutorial on how to construct a morphological analyzer for a language using finite-state techniques. A small toy grammar of English noun and verb inflection is built step-by-step to illustrate overall design issues. While the grammar is small, much larger grammars can be built using the same design principles. Basic familiarity with regular expressions and foma is assumed, such as is outlined in the Getting started with foma page.\n",
    "\n",
    "## Definition\n",
    "Since a \"morphological analyzer\" could mean any number of things, let's first settle on a task description and define what the morphological analyzer is supposed to accomplish. In this implementation, a morphological analyzer is taken to be a black box (as in Fig. 1), which happens to be implemented as a finite-state transducer, that translates word forms (such as runs) into a string that represents its morphological makeup, such as run+V+3p+Sg: a verb in the third person singular present tense.\n",
    "\n",
    "\n",
    "\n",
    "Naturally, if the word form is ambiguous (as runs is), the job of the analyzer is to output all tag sequences consistent with the grammar and the input word. In the above example, the transducer should perhaps also output run+N+Pl, or some similar sequence to convey the possibility of a noun reading of runs. Since finite-state transducers are inherently bidirectional devices, i.e. we can run a transducer in the inverse direction as well as the forward direction, the same FST, once we've built it, can serve both as a generator and an analyzer. The standard practice is to build morphological transducers so that the input (or domain) side is the analysis side, and the output (or range) side contains the word forms.\n",
    "\n",
    "In real life, morphological analyzers tend to provide much more detailed information than this. Figure 2 shows a more elaborate analyzer's output for Basque with the input work maiatzaren, together with an illustration about how a feature matrix can be recovered from the string output of the analyzer.\n",
    "\n",
    "\n",
    "\n",
    "The goal is then is build a finite-state transducer that accomplishes this string-to-string mapping of analyses to surface forms and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design\n",
    "The construction of the final transducer will be broken down into two large components:\n",
    "\n",
    "- A lexicon/morphotactics part\n",
    "- A phonological/morphophonological/orthographic part\n",
    "\n",
    "### The lexicon\n",
    "The first component, which we call the lexicon component, will be a transducer that:\n",
    "\n",
    "- Accepts as input only the valid stems/lemmas of the language, followed by only a legal sequence of tags.\n",
    "- Produces as output from these, an intermediate form, where the tags are replaced by the morphemes that they correspond to.\n",
    "- May produce additional symbols in the output, such as special symbols that serve to mark the presence of morpheme boundaries.\n",
    "For example, in the analyzer about to be constructed, the lexicon component FST will perform the following mappings:\n",
    "\n",
    "```\n",
    "c a t +N +Pl      w a t c h +N +Pl      w a t c h +V +3P +Sg     (input side)\n",
    "c a t ^  s        w a t c h ^  s        w a t c h ^  s           (output side)\n",
    "```\n",
    "\n",
    "There are two things to note here. The first is that we are using the symbol ^ to mark a morpheme boundary. The second is that while each letter in the stem is represented by its own symbol (`w,a,t,c,h,` etc.), each complete tag is one separate symbol, a multicharacter symbol (`+N, +Pl,` etc.) The spaces in the above show the symbol boundaries to illustrate this. Figure 3 shows what a lexicon transducer that only encoded these three words would look like. Naturally, we will have some more features and a larger lexicon in what is described below.\n",
    "\n",
    "\n",
    "The part that accomplishes this, the lexicon transducer, will be written in a formalism called lexc. While it is possible to construct the lexicon transducer through regular expressions in foma, the lexc-formalism is more suited for lexicon construction and expressing morphotactics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hfst\n",
    "import fstutils as fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function remove_epsilons in module fstutils:\n",
      "\n",
      "remove_epsilons(string, epsilon='@_EPSILON_SYMBOL_@')\n",
      "    Removes the epsilon transitions from the string along a path from hfst.\n",
      "    \n",
      "    Args:\n",
      "        string (str): The string (e.g. input path, output form) from which the epsilons should be deleted.\n",
      "        epsilon (str, optional):  The epsilon string to remove. Defaults to the default setting in hfst,\n",
      "        '@_EPSILON_SYMBOL_@'. Pass this only if you've redefined the epsilon symbol string in hfst.\n",
      "    \n",
      "    Returns:\n",
      "        str: The desired string, without epsilons\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fst.remove_epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = fst.Definitions({\n",
    "    \"V\": \"[a|i|e|o|u]\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = hfst.compile_lexc_file('english.lexc')\n",
    "\n",
    "consonantduplication = hfst.regex(defs.replace('g -> g g || _ \"^\" [i n g | e d]'))\n",
    "edeletion = hfst.regex('e -> 0 || _ \"^\" [ i n g | e d ]')\n",
    "einsertion = hfst.regex('[..] -> e || s | z | x | c h | s h _ \"^\" s')\n",
    "yreplacement = hfst.regex('y -> i e || _ \"^\" s ,, y-> i || _ \"^\" e d')\n",
    "kinsertion = hfst.regex(defs.replace('[..] -> k || V c _ \"^\" [e d | i n g]'))\n",
    "cleanup = hfst.regex('\"^\" -> 0')\n",
    "\n",
    "# be careful, since composition is done in place, rerunning composes without redefining the fst from scratch will make mega-fsts\n",
    "\n",
    "grammar.compose(consonantduplication)\n",
    "grammar.compose(einsertion)\n",
    "grammar.compose(edeletion)\n",
    "grammar.compose(yreplacement)\n",
    "grammar.compose(kinsertion)\n",
    "grammar.compose(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'panicked'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst.lookup(grammar, 'panic+V+Past')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beg+V:\n",
      " beg\n",
      "beg+V+3P+Sg:\n",
      " begs\n",
      "beg+V+Past:\n",
      " begged\n",
      "beg+V+PastPart:\n",
      " begged\n",
      "beg+V+PresPart:\n",
      " begging\n",
      "cat+N+Sg:\n",
      " cat\n",
      "cat+N+Pl:\n",
      " cats\n",
      "city+N+Pl:\n",
      " cities\n",
      "city+N+Sg:\n",
      " city\n",
      "fox+N+Sg:\n",
      " fox\n",
      "fox+V:\n",
      " fox\n",
      "fox+V+Past:\n",
      " foxed\n",
      "fox+V+PastPart:\n",
      " foxed\n",
      "fox+V+PresPart:\n",
      " foxing\n",
      "fox+N+Pl:\n",
      " foxes\n",
      "fox+V+3P+Sg:\n",
      " foxes\n",
      "make+V+Past:\n",
      " maked\n",
      "make+V+PastPart:\n",
      " maked\n",
      "make+V+PresPart:\n",
      " making\n",
      "make+V:\n",
      " make\n",
      "make+V+3P+Sg:\n",
      " makes\n",
      "panic+N+Sg:\n",
      " panic\n",
      "panic+N+Pl:\n",
      " panics\n",
      "panic+V:\n",
      " panic\n",
      "panic+V+3P+Sg:\n",
      " panics\n",
      "panic+V+Past:\n",
      " panicked\n",
      "panic+V+PastPart:\n",
      " panicked\n",
      "panic+V+PresPart:\n",
      " panicking\n",
      "try+V+Past:\n",
      " tried\n",
      "try+V+PastPart:\n",
      " tried\n",
      "try+N+Pl:\n",
      " tries\n",
      "try+V+3P+Sg:\n",
      " tries\n",
      "try+N+Sg:\n",
      " try\n",
      "try+V:\n",
      " try\n",
      "try+V+PresPart:\n",
      " trying\n",
      "watch+N+Sg:\n",
      " watch\n",
      "watch+V:\n",
      " watch\n",
      "watch+V+Past:\n",
      " watched\n",
      "watch+V+PastPart:\n",
      " watched\n",
      "watch+V+PresPart:\n",
      " watching\n",
      "watch+N+Pl:\n",
      " watches\n",
      "watch+V+3P+Sg:\n",
      " watches\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fst.pairs(grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
