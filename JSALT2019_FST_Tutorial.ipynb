{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological analysis with FSTs\n",
    "The following is a brief and basic tutorial on how to construct a **morphological analyzer** for a language using finite-state techniques. A toy grammar of English noun and verb inflections is built step-by-step to illustrate overall design issues. While the grammar is small, much larger grammars can be built using the same design principles. This tutorial uses the [Helsinki Finite-State Transducer toolkit](http://hfst.github.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hfst\n",
    "import fstutils as fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function remove_epsilons in module fstutils:\n",
      "\n",
      "remove_epsilons(string, epsilon='@_EPSILON_SYMBOL_@')\n",
      "    Removes the epsilon transitions from the string along a path from hfst.\n",
      "    \n",
      "    Args:\n",
      "        string (str): The string (e.g. input path, output form) from which the epsilons should be deleted.\n",
      "        epsilon (str, optional):  The epsilon string to remove. Defaults to the default setting in hfst,\n",
      "        '@_EPSILON_SYMBOL_@'. Pass this only if you've redefined the epsilon symbol string in hfst.\n",
      "    \n",
      "    Returns:\n",
      "        str: The desired string, without epsilons\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fst.remove_epsilons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design\n",
    "The construction of the final transducer is broken down into two large components:\n",
    "\n",
    "- A lexicon\n",
    "- Alternation rules\n",
    "\n",
    "We combine the lexicon FST and the various FSTs that encode alternation rules into one large transducer that acts like a cascade. This single large transducer has the same effect as providing an input to the lexicon transducer, taking its output and feeding it into the first rule transducer, taking its output and feeding it into the next rule transducer, and so on.\n",
    "This cascade is accomplished by the regular expression composition operator (`.o.`). Suppose we have the lexicon transducer in an FST named `Lexicon` and the various alternation rules as FSTs named `Rule1`, ..., `RuleN`. We can issue the regular expression\n",
    "```\n",
    "Lexicon .o. Rule1 .o. Rule2 .o. ... .o. RuleN ;\n",
    "```\n",
    "and produce a single transducer that is the composite of the different rule transducers and the lexicon transducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = fst.Definitions({\n",
    "    \"V\": \"[a|i|e|o|u]\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = hfst.compile_lexc_file('english.lexc')\n",
    "\n",
    "consonantduplication = hfst.regex(defs.replace('g -> g g || _ \"^\" [i n g | e d]'))\n",
    "edeletion = hfst.regex('e -> 0 || _ \"^\" [ i n g | e d ]')\n",
    "einsertion = hfst.regex('[..] -> e || s | z | x | c h | s h _ \"^\" s')\n",
    "yreplacement = hfst.regex('y -> i e || _ \"^\" s ,, y-> i || _ \"^\" e d')\n",
    "kinsertion = hfst.regex(defs.replace('[..] -> k || V c _ \"^\" [e d | i n g]'))\n",
    "cleanup = hfst.regex('\"^\" -> 0')\n",
    "\n",
    "# be careful, since composition is done in place, rerunning composes without redefining the fst from scratch will make mega-fsts\n",
    "\n",
    "grammar.compose(consonantduplication)\n",
    "grammar.compose(einsertion)\n",
    "grammar.compose(edeletion)\n",
    "grammar.compose(yreplacement)\n",
    "grammar.compose(kinsertion)\n",
    "grammar.compose(cleanup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'panicked'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fst.lookup(grammar, 'panic+V+Past')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beg+V:\n",
      " beg\n",
      "beg+V+3P+Sg:\n",
      " begs\n",
      "beg+V+Past:\n",
      " begged\n",
      "beg+V+PastPart:\n",
      " begged\n",
      "beg+V+PresPart:\n",
      " begging\n",
      "cat+N+Sg:\n",
      " cat\n",
      "cat+N+Pl:\n",
      " cats\n",
      "city+N+Pl:\n",
      " cities\n",
      "city+N+Sg:\n",
      " city\n",
      "fox+N+Sg:\n",
      " fox\n",
      "fox+V:\n",
      " fox\n",
      "fox+V+Past:\n",
      " foxed\n",
      "fox+V+PastPart:\n",
      " foxed\n",
      "fox+V+PresPart:\n",
      " foxing\n",
      "fox+N+Pl:\n",
      " foxes\n",
      "fox+V+3P+Sg:\n",
      " foxes\n",
      "make+V+Past:\n",
      " maked\n",
      "make+V+PastPart:\n",
      " maked\n",
      "make+V+PresPart:\n",
      " making\n",
      "make+V:\n",
      " make\n",
      "make+V+3P+Sg:\n",
      " makes\n",
      "panic+N+Sg:\n",
      " panic\n",
      "panic+N+Pl:\n",
      " panics\n",
      "panic+V:\n",
      " panic\n",
      "panic+V+3P+Sg:\n",
      " panics\n",
      "panic+V+Past:\n",
      " panicked\n",
      "panic+V+PastPart:\n",
      " panicked\n",
      "panic+V+PresPart:\n",
      " panicking\n",
      "try+V+Past:\n",
      " tried\n",
      "try+V+PastPart:\n",
      " tried\n",
      "try+N+Pl:\n",
      " tries\n",
      "try+V+3P+Sg:\n",
      " tries\n",
      "try+N+Sg:\n",
      " try\n",
      "try+V:\n",
      " try\n",
      "try+V+PresPart:\n",
      " trying\n",
      "watch+N+Sg:\n",
      " watch\n",
      "watch+V:\n",
      " watch\n",
      "watch+V+Past:\n",
      " watched\n",
      "watch+V+PastPart:\n",
      " watched\n",
      "watch+V+PresPart:\n",
      " watching\n",
      "watch+N+Pl:\n",
      " watches\n",
      "watch+V+3P+Sg:\n",
      " watches\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(fst.pairs(grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
